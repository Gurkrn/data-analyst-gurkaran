![AWS_Academy_Graduate___AWS_Academy_Cloud_Foundations_Badge20250327-27-qhsyw2_page-0001](https://github.com/user-attachments/assets/2354af89-ab75-4a71-8ffc-ef6590f3d683)
# data-analyst-gurkaran
# Project Title: Titanic Survival Analysis
Objective and Aim
This project aims to analyze the survival rates of Titanic passengers based on various factors such as passenger class, gender, and overall demographics. Using exploratory data analysis (EDA) and visualization techniques, we identify key trends and patterns in survival outcomes, providing insights into the factors that influenced survival during the disaster.

Methodology
The dataset used for analysis is the Titanic dataset, which contains details about passengers, including their class, gender, age, fare, and survival status.

Data preprocessing involved handling missing values, categorizing variables, and performing statistical calculations.

Exploratory Data Analysis (EDA) was conducted using Python libraries such as Pandas, Seaborn, Matplotlib, and Plotly to visualize survival distributions.

A dashboard was built using Dash and Plotly to present survival analysis interactively.

Key visualizations include:

Overall Survival Rate (Pie Chart) â€“ Displays the proportion of survivors and non-survivors.

Survival Rate by Passenger Class (Bar Chart) â€“ Illustrates how survival rates differed across first, second, and third-class passengers.

Results and Findings
    The overall survival rate was 36.4%, while 63.6% of passengers did not survive.

Survival rates by passenger class:

    First-class: 46.7% survival rate.

    Second-class: 32.3% survival rate.

    Third-class: 33.0% survival rate.

First-class passengers had the highest survival rate, suggesting a strong correlation between socioeconomic status and survival probability.

Women and children had a significantly higher survival rate, reinforcing the "women and children first" evacuation policy.

Third-class passengers had the lowest survival rates, likely due to their cabins being in the lower decks, limiting accessibility to lifeboats.

## Visualizations

### 1. Overall Survival Distribution  
![Overall Survival Distribution](https://github.com/Gurkrn/data-analyst-gurkaran/blob/84cfd24633ca0d2ee40aac364ae0ea1ce974672d/newplot.png?raw=true)

### 2. Survival Rate by Passenger Class  
![Survival by Class](https://github.com/Gurkrn/data-analyst-gurkaran/blob/84cfd24633ca0d2ee40aac364ae0ea1ce974672d/Survival%20pass.png?raw=true)


Conclusion
This analysis demonstrates how socioeconomic status, gender, and class significantly influenced survival on the Titanic. The findings provide valuable insights into historical survival patterns and highlight the potential for further predictive modeling. Future work could involve applying machine learning algorithms to predict survival outcomes based on passenger attributes, enhancing data-driven decision-making in similar scenarios.

# PROJECT 2


# Understanding Customer Purchase Patterns at XYZ Retail
Objective
The goal of this project is to conduct a descriptive analysis of customer purchase patterns at XYZ Retail. By analyzing transaction data, we aim to identify key trends, consumer behaviors, and insights that can inform marketing strategies and inventory management.

Methodology
Data Collection & Preparation

Generated a synthetic dataset of 1,000+ transaction records with key details such as Transaction ID, Customer ID, Purchase Date, Product Category, Quantity, Price, Payment Method, and Store Location.

Cleaned the data by addressing missing values and correcting data types.

Descriptive Analysis & Visualization

Sales Trend Over Time: Analyzed total transactions per month to identify peak shopping periods.

Top Product Categories: Identified the most frequently purchased product categories.

Payment Method Distribution: Examined customer preferences for different payment options.

Customer Segmentation & Insights

Segmented customers based on purchase frequency and behavior.

Derived insights to optimize inventory, promotions, and marketing strategies.

Results & Findings
ðŸ“Š Sales Trend Over Time

The highest number of transactions occurred in August 2023 (104 transactions), indicating a peak shopping period, possibly due to seasonal promotions.

### **1. Sales Trend Over Time**  
![Sales Trend Over Time](https://github.com/Gurkrn/data-analyst-gurkaran/blob/383ad10e6ea53f4404015d7f66c3abac50ea457b/newplot%20(1).png?raw=true)  


ðŸ›’ Top Product Categories
The most purchased category was Furniture (226 purchases), followed by Toys (209), Clothing (203), Electronics (193), and Groceries (169).

### **2. Product Category Distribution**  
![Product Category Distribution](https://github.com/Gurkrn/data-analyst-gurkaran/blob/383ad10e6ea53f4404015d7f66c3abac50ea457b/newplot%20(2).png?raw=true)  

ðŸ’³ Payment Method Preferences
Digital Payments (344 transactions) were the most used method, slightly ahead of Cash (335 transactions) and Credit Cards (321 transactions).

### **3. Payment Method Preferences**  
![Payment Method Preferences](https://github.com/Gurkrn/data-analyst-gurkaran/blob/383ad10e6ea53f4404015d7f66c3abac50ea457b/newplot%20(3).png?raw=true) 

# Key Insights & Recommendations

âœ… Peak Shopping Periods: August had the highest transactions, suggesting seasonal trends that XYZ Retail can leverage through targeted marketing campaigns.

âœ… Inventory Planning: Furniture and Toys had the highest purchase volumes, indicating a need for adequate stock levels and possible cross-promotions between these categories.

âœ… Payment Preferences: The shift toward digital payments suggests an opportunity to introduce loyalty rewards or exclusive discounts for online or mobile transactions.

By leveraging these insights, XYZ Retail can optimize inventory management, pricing strategies, and customer engagement, ultimately improving sales and customer satisfaction. ðŸš€


# Project 3

# Diagnostic Analysis of Sales Decline at XYZ Retail

Objective & Aim

This project investigates the reasons behind the decline in sales at XYZ Retail over the past year. By analyzing sales trends, customer behavior, and transaction data, the goal is to identify key patterns and correlations that explain the downturn. The findings will provide actionable recommendations for improving sales performance and optimizing business strategies.

Methodology
Trend Analysis

Examined monthly sales trends to identify periods with the most significant sales declines.

Correlation Analysis

Conducted statistical regression analysis to measure the relationship between sales and variables such as time, inventory levels, customer behavior, and external factors.

Root Cause Analysis

Used techniques such as the "5 Whys" and Fishbone Diagram to explore potential causes, including operational inefficiencies, competitive influences, and consumer behavior shifts.

Segmentation Analysis

Categorized customers based on spending behavior and location to determine which groups are most affected by the decline.

Results & Key Findings
Sales have shown a steady decline over time, with an estimated monthly drop of approximately $1,317, as confirmed by regression analysis (p = 0.007).

The most significant declines were observed in July and November, indicating possible seasonal trends or external economic factors.

Furniture and Electronics were the best-selling product categories, with total sales of $83,637 and $77,682, respectively.

Digital payments accounted for the highest transaction amounts ($134,767), suggesting a shift toward online and cashless payments.

Customers in Chicago and Los Angeles had the highest average transaction values (~$267), while those in New York had the lowest ($243), highlighting regional differences in purchasing behavior.

Visualizations & Findings
Sales Decline Over Time
Monthly sales data revealed a downward trend, with the most significant declines observed in July and November.

Regression analysis indicated a strong negative correlation between time and sales, with an average monthly decrease of approximately $1,317.

### **1. Sales Trend Over Time**  
![Sales Trend Over Time](https://github.com/Gurkrn/data-analyst-gurkaran/blob/1038402cccc4b922a73e508129062cb4d2bee29b/newplot%20(4).png?raw=true)  

Sales vs. Other Factors
Regression results suggest that time is a significant predictor of sales decline (p = 0.007).

External factors, such as inventory levels and changing consumer spending habits, may also contribute to the decline.

### **2. Correlation Analysis**  
![Correlation Analysis](https://github.com/Gurkrn/data-analyst-gurkaran/blob/1038402cccc4b922a73e508129062cb4d2bee29b/Corre.jpg?raw=true)  


Potential causes identified include seasonal demand fluctuations, shifts in payment preferences, and regional differences in customer spending.

Further qualitative research, such as staff interviews and customer feedback analysis, could provide deeper insights.

### **3. Payment Method Preferences**  
![Payment Method Preferences](https://github.com/Gurkrn/data-analyst-gurkaran/blob/1038402cccc4b922a73e508129062cb4d2bee29b/newplot%20(6).png?raw=true)  


High-spending customers were primarily located in Chicago and Los Angeles, with average transaction values above $260.

New York customers had the lowest average spending (~$243), suggesting potential regional differences in purchasing power or preferences.

### **4. Customer Segmentation**  
![Customer Segmentation](https://github.com/Gurkrn/data-analyst-gurkaran/blob/1038402cccc4b922a73e508129062cb4d2bee29b/newplot%20(5).png?raw=true)  

Conclusion & Recommendations

Implement targeted pricing strategies and promotions during months with the steepest declines.

Strengthen digital payment and e-commerce capabilities to cater to changing customer preferences.

Develop location-specific marketing campaigns to improve sales in underperforming regions such as New York.

Conduct customer surveys to gain qualitative insights into shifting consumer preferences.

Optimize inventory management to ensure product availability aligns with demand patterns.


# PROJECT 4

# Data Wrangling for Enhanced Customer Analytics at XYZ Company

Project Objective
The primary objective of this project is to clean, transform, and consolidate customer sales data to enhance its accuracy and usability for analysis at XYZ Company. The dataset, sourced from a supermarket sales record, contains inconsistencies, missing values, and formatting issues that need to be addressed for effective business insights.

Project Aim
By conducting a comprehensive data wrangling process, this project aims to prepare a structured dataset that can be used for customer behavior analysis, sales forecasting, and business decision-making. The cleaned dataset will serve as a foundation for further data-driven strategies, improving XYZ Company's ability to identify key sales trends and optimize marketing efforts.

Methodology
1. Data Collection
The dataset was sourced from Kaggleâ€™s Supermarket Sales Dataset and imported into Google Colab for processing.

2. Data Assessment
Conducted an initial analysis to detect:

Missing values

Duplicate records

Inconsistent data formats

Outliers and discrepancies in numerical fields

3. Data Cleaning
Missing Values: Handled using appropriate imputation techniques or removal where necessary.

Duplicate Records: Removed to maintain data integrity.

Inconsistent Formats: Standardized date columns and categorical variables for uniformity.

Outliers: Detected and addressed where necessary, ensuring data reliability.

4. Data Transformation
Converted data types (e.g., Date column formatted as datetime).

Engineered new features, including Total Revenue per Product Line and Customer Spending Patterns.

Aggregated sales data to provide insights at a store level, category level, and customer level.

5. Data Consolidation
Merged relevant attributes to create a single structured dataset that connects sales transactions, customer demographics, and purchase patterns.

6. Documentation & Validation
Documented every step of the data wrangling process, including data sources, cleaning methods, and transformations.

Conducted Exploratory Data Analysis (EDA) to validate the cleaned dataset, ensuring completeness and accuracy.

Results & Findings
1. Data Quality Improvements

ðŸ”¹ Duplicates Removed: The dataset originally contained redundant entries that were eliminated.

ðŸ”¹ Missing Data Handled: Missing values were either imputed or removed based on relevance.

ðŸ”¹ Standardized Formats: Date, categorical, and numerical fields were cleaned and structured properly.

2. Key Insights from Cleaned Data
   
ðŸ“Š Customer Spending Trends: Identified high-revenue product lines and top-performing store branches.

ðŸ“Š Sales Performance Analysis: Aggregated sales data provided store-level and category-level insights.

ðŸ“Š Time-Based Sales Trends: Validated peak sales periods, helping optimize marketing and inventory decisions.

Visualizations

Before data cleaning, the dataset contained multiple inconsistencies, which affected analysis.
### **1. Uncleaned Dataset Overview**  
![Uncleaned Dataset Overview](https://github.com/Gurkrn/data-analyst-gurkaran/blob/7e16bd24c18c11c5e6bab1b0382e13ee95a05183/DW.jpg)  

### **2. Cleaned Dataset Overview**  
![Cleaned Dataset](https://github.com/Gurkrn/data-analyst-gurkaran/blob/7e16bd24c18c11c5e6bab1b0382e13ee95a05183/DW2.jpg)  

After data wrangling, the dataset is structured, clean, and ready for insights.




# PROJECT 4

# Data Control 

Exploratory Data Analysis (EDA) and Key Performance Indicators (KPIs) for Data Quality Control
Maintaining high-quality data is an ongoing process that requires systematic evaluation, cleaning, and monitoring. This section provides a detailed exploratory data analysis (EDA) with references to our previous project, Data Wrangling for Enhanced Customer Analytics, while outlining a robust Data Quality Control framework.

In our previous project, data wrangling and cleaning revealed several data quality challenges that significantly impacted the accuracy, completeness, and reliability of business insights. One major issue was handling missing values, where we identified gaps in key fields such as customer demographics and purchase amounts. To address this, we applied various imputation techniques, including mean, median, and logical filling based on business rules. Now, in the current Data Quality Control project, we establish real-time missing value detection to ensure data completeness from the outset.

Duplicate records posed another critical issue in our initial dataset. In Project 1, we found 1,245 duplicate entries, which skewed analytics and resulted in misleading trends. Using Pandasâ€™ .duplicated() method and fuzzy matching techniques, we detected and removed these redundant records. Moving forward, automated deduplication scripts will be integrated into our Data Quality Monitoring system to prevent duplicates from entering the database.

Another challenge encountered in Project 1 was inconsistent data formats, particularly in date fields. The dataset contained multiple formats such as MM/DD/YYYY and YYYY-MM-DD, causing errors in trend analysis plots. To resolve this, we standardized all date formats using Pythonâ€™s datetime module and applied validation rules to enforce uniformity. The current Data Quality Control framework now ensures strict adherence to format guidelines for all new data entries, reducing errors and ensuring consistency across datasets.

Validating data accuracy was a crucial step in ensuring that calculations aligned correctly. Initially, we found that transaction values did not always match computed totals, signaling potential data entry errors. To address this, we cross-validated numerical fields by recalculating totals using unit price and quantity. This process helped identify discrepancies and led to necessary corrections. As part of the current project, automated validation checks will be implemented before data is accepted into the system, preventing such issues in future datasets.

Standardizing categorical data was another key aspect of improving data quality. In Project 1, we observed inconsistent labels, such as variations in spelling and capitalization for payment methods (e.g., "Credit Card" vs. "credit_card"), which made categorical analysis unreliable. To address this, we applied label standardization techniques and enforced lookup tables to maintain uniformity. Moving forward, the Data Quality Control framework includes controlled vocabulary and dropdown menus to ensure consistency at the point of data entry.

To continuously monitor and improve data quality, we define several key performance indicators (KPIs). These include the percentage of missing data, the number of duplicate records, the format consistency score, the error rate in key fields, validation rule compliance, and the timeliness of data entry. For instance, missing data percentage is tracked to ensure it remains below 1%, duplicate records should be completely eliminated, and validation rule compliance should exceed 95%. These KPIs provide measurable benchmarks to assess and improve data quality over time.

Exploratory data analysis plays a crucial role in detecting quality issues early and ensuring that cleaned data is ready for analysis. In Project 1, we used visualizations such as boxplots to detect outliers in transaction amounts, count plots to identify duplicate records, and bar charts to visualize inconsistencies in product categories. The uncleaned dataset exhibited a range of issues, including extreme values and formatting inconsistencies. After data wrangling, histograms and scatterplots confirmed that the dataset was structured, clean, and ready for insights.

The cleaned dataset provides a foundation for high-quality analysis, ensuring reliable business insights. By integrating automated validation, monitoring dashboards, and best practices, the Data Quality Control framework enhances data integrity and supports informed decision-making. The transition from reactive data cleaning to proactive quality control measures marks a significant improvement in the organizationâ€™s data management approach.


Key Performance Indicators (KPIs) for Data Quality Control
To effectively measure data quality improvements and ensure consistency across datasets, we define the following KPIs. These indicators provide quantitative benchmarks for assessing the success of data quality control measures and allow continuous monitoring of data integrity. The table below outlines the key KPIs along with their target values, methods of calculation, and Python code suggestions to implement them.


## **Key Performance Indicators (KPIs) for Data Quality Control**

| **KPI**                          | **Description**                                                   | **Target Value** | **Calculation Method**  | **Python Code Suggestion** |
|----------------------------------|-----------------------------------------------------------------|---------------|---------------------|----------------------|
| **Missing Data Percentage**      | Measures the percentage of missing values in critical columns.  | < 1%         | (Missing values / Total records) * 100  | `df.isnull().sum() / len(df) * 100` |
| **Duplicate Records**            | Counts the number of duplicate rows.                            | 0            | `df.duplicated().sum()`  | `df[df.duplicated()]` |
| **Data Format Consistency**      | Ensures consistent formats in date and categorical fields.      | 100%         | Checking format compliance  | `pd.to_datetime(df['date_column'], errors='coerce')` |
| **Validation Rule Compliance**   | Percentage of records that meet predefined validation rules.     | > 95%        | (Valid records / Total records) * 100  | `df[df['amount'] > 0].count() / len(df) * 100` |
| **Data Entry Accuracy**          | Identifies inconsistencies in manually entered data.            | < 2% error   | Comparing actual vs. expected values  | `df['total'] == df['price'] * df['quantity']` |
| **Data Standardization Score**   | Measures adherence to predefined naming conventions.            | > 98%        | Ratio of standardized records  | `df['category'].str.lower().nunique() / df['category'].nunique() * 100` |
| **Timeliness of Data Entry**     | Evaluates how quickly data is entered after an event occurs.    | < 24 hours   | Time difference between event and entry  | `df['entry_time'] - df['event_time']` |
| **Outlier Detection Rate**       | Identifies extreme values that may indicate errors.             | < 5%         | Z-score or IQR method  | `zscore(df['column']) > 3` |
| **Data Completeness Score**      | Measures the presence of all required fields.                   | > 99%        | (Complete records / Total records) * 100  | `df.dropna().shape[0] / df.shape[0] * 100` |

Analysis and Explanation
In our previous data wrangling project, we identified key data quality issues such as missing values, duplicates, inconsistent formats, and inaccurate calculations. The current Data Quality Control initiative builds on these findings by implementing structured monitoring and validation processes.

One critical improvement is the proactive detection of missing values, reducing them from a high percentage in Project 1 to an acceptable threshold of less than 1%. The code implementation ensures that missing data is identified early and addressed through appropriate imputation techniques.

Duplicate records were another significant issue in our prior dataset, leading to incorrect aggregations in sales and customer behavior analysis. Through the implementation of automated deduplication checks, the current project eliminates redundancy before it impacts analysis.

Inconsistent data formats, especially in dates and categorical values, previously led to difficulties in filtering and aggregating data. The validation scripts now ensure that all new data adheres to a standardized format, significantly improving consistency and usability.

By defining KPIs such as validation rule compliance and data entry accuracy, we ensure that errors in manually entered data are minimized. This approach allows for real-time correction mechanisms that prevent faulty records from affecting decision-making.

Timeliness of data entry is another essential metric, ensuring that data is updated within 24 hours of an event. In the previous project, delayed data entry led to outdated insights, impacting business decisions. The new framework automates tracking and alerts when data entry lags behind the expected time frame.

Outlier detection is also crucial, as extreme values can distort trends and lead to misleading conclusions. The implementation of the Z-score method ensures that anomalies are flagged and reviewed before they influence critical analyses.

Data completeness, one of the most fundamental KPIs, measures whether all necessary fields are populated. In Project 1, incomplete records affected segmentation and trend analysis. The current framework ensures that completeness is maintained above 99%, improving the reliability of insights.

By consistently tracking and analyzing these KPIs, we ensure that our datasets remain accurate, reliable, and ready for business intelligence applications. The transition from reactive data cleaning to proactive data quality management marks a significant advancement in our data governance strategy.








